{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8833ce95-c288-4f77-a0f8-79be647bd34e",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8022446-2fe4-4bee-b5f5-3325b40c70e6",
   "metadata": {},
   "source": [
    "The objective of this notebook is to fine-tune the hyperparameters of the ideal configuration identified during the model induction phase. To do this, the main hyperparameters of the TF-IDF transformer and the SVM classifier will be adjusted. Performance evaluation will be performed using hierarchical classification metrics.<br>\n",
    "\n",
    "**Source file:** select_202425091103-translated.csv<br>\n",
    "**Destination file:** select_202425091103-TFIDF LCPN SVM.pickle<br>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f057ec-f5ca-476a-9ce5-b9f28b6d5916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a095ce5e-de79-4c31-908e-1bf4f9e1ab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnpq = ['cnpq_area_level_1',\n",
    "        'cnpq_area_level_2',\n",
    "        'cnpq_area_level_3',\n",
    "        'cnpq_area_level_4']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d452276a-f388-494f-a523-6982bf33a957",
   "metadata": {},
   "source": [
    "## Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0308acd1-28db-4222-a3cd-ec40e7699b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f54e42-8811-4e2f-8753-171075280084",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../preprocessed/select_202425091103-translated.csv', dtype=str, na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e91ed09-b920-472d-8a6e-0b8ed6272cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c38c7a8-a79e-4f6e-8796-48c610a5740c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1764a8f-3cfb-4998-986b-177e90b5dd0a",
   "metadata": {},
   "source": [
    "## Loading the Transformation Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a29798-930e-4aae-9931-c05db0713c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/embedding.py\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class Normalizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "\n",
    "    def pre_processing(self, doc):\n",
    "        pattern = re.compile(r'\\d+|http\\S+|<.*?>', re.IGNORECASE)\n",
    "        return pattern.sub('', doc).lower()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [' '.join(self.stemmer.stem(word) for word in word_tokenize(self.pre_processing(doc))) for doc in X]\n",
    "\n",
    "class Lazy(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, vectorizer, ngram_range, max_df, min_df, max_features):\n",
    "        self.vectorizer = vectorizer(ngram_range=ngram_range, max_df=max_df, min_df=min_df, max_features=max_features)\n",
    "        self.ngram_range = ngram_range\n",
    "        self.max_df = max_df\n",
    "        self.min_df = min_df\n",
    "        self.max_features = max_features\n",
    "\n",
    "    def fit(self, raw_documents, y=None):\n",
    "        self.vectorizer.fit(raw_documents, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, raw_documents):\n",
    "        return self.vectorizer.transform(raw_documents).toarray()\n",
    "\n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        return self.vectorizer.fit_transform(raw_documents, y).toarray()\n",
    "\n",
    "class BoW(Lazy):\n",
    "    def __init__(self, ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=9000):\n",
    "        super().__init__(CountVectorizer, ngram_range=ngram_range, max_df=max_df, min_df=min_df, max_features=max_features)\n",
    "\n",
    "class TFIDF(Lazy):\n",
    "    def __init__(self, ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=9000):\n",
    "        super().__init__(TfidfVectorizer, ngram_range=ngram_range, max_df=max_df, min_df=min_df, max_features=max_features)\n",
    "\n",
    "class Embedding(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = SentenceTransformer('../models/' + model_name)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([self.model.encode(text) for text in X])\n",
    "\n",
    "class RoBERTa(Embedding):\n",
    "    def __init__(self, model_name='all-distilroberta-v1'):\n",
    "        super().__init__(model_name=model_name)\n",
    "\n",
    "class USE(Embedding):\n",
    "    def __init__(self, model_name='distiluse-base-multilingual-cased-v1'):\n",
    "        super().__init__(model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44abcb2a-1c27-4c64-a87a-2d890d55be47",
   "metadata": {},
   "source": [
    "## Loading the SVM wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0740fd-917a-44d3-9fb0-55f7b3e12ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/svm.py\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "class SVM(SVC):\n",
    "    def __init__(self, C=1.0, kernel='rbf', probability=True):\n",
    "        super().__init__(C=C, kernel=kernel, probability=probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c53bbe0-f832-4189-b5f9-93ddcaa5f0eb",
   "metadata": {},
   "source": [
    "## Defining the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf196940-d7ac-487e-b884-581e06e266d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hiclass import LocalClassifierPerNode\n",
    "from hiclass.metrics import f1\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d54035-66e0-43e6-9ab1-826c235bcbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('', Normalizer()),\n",
    "    ('TFIDF', TFIDF()),\n",
    "    ('LCPN', LocalClassifierPerNode(SVM()))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9e0fa6-dc09-4662-908b-37060ecc36d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'TFIDF__max_df': [0.75, 1.0],\n",
    "    'TFIDF__min_df': [1, 3],\n",
    "    'TFIDF__max_features': [5000, 7000, 9000],\n",
    "    'TFIDF__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'LCPN__local_classifier__C': [0.1, 1, 10],\n",
    "    'LCPN__local_classifier__kernel': ['linear', 'rbf'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64529a66-4709-406b-9585-6da878a9e576",
   "metadata": {},
   "source": [
    "## Splitting training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fac231-abfa-46be-b983-a283e2a9f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['all'].to_numpy(), df[cnpq].to_numpy(), test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523d041e-3d34-405b-93f1-74af7d2f7552",
   "metadata": {},
   "source": [
    "## Running the Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec32a0d1-7213-4912-a375-014d55e4d839",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=20, cv=5, verbose=1, scoring=make_scorer(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4c9c8e-07b1-4e7f-bb91-09df18f7bf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea059a3-2099-4e4f-a91a-7c4b61e29297",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Melhores par√¢metros:\", grid_search.best_params_)\n",
    "print(\"Melhor cross-validation score:\", grid_search.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
